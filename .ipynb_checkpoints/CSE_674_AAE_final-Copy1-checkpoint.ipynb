{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phaniram/anaconda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import mnist_data\n",
    "#import prior_factory as prior\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_SIZE_MNIST = 28\n",
    "SOURCE_URL = 'http://yann.lecun.com/exdb/mnist/'\n",
    "DATA_DIRECTORY = \"data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_MNIST_data(use_norm_shift=False, use_norm_scale=True, use_data_augmentation=False):\n",
    "    # Get the data.\n",
    "    train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "    train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "    test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "    test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "    # Extract it into numpy arrays.\n",
    "    train_data = extract_data(train_data_filename, 60000, use_norm_shift, use_norm_scale)\n",
    "    train_labels = extract_labels(train_labels_filename, 60000)\n",
    "    test_data = extract_data(test_data_filename, 10000, use_norm_shift, use_norm_scale)\n",
    "    test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "    # Generate a validation set.\n",
    "    validation_data = train_data[:VALIDATION_SIZE, :]\n",
    "    validation_labels = train_labels[:VALIDATION_SIZE,:]\n",
    "    train_data = train_data[VALIDATION_SIZE:, :]\n",
    "    train_labels = train_labels[VALIDATION_SIZE:,:]\n",
    "\n",
    "    # Concatenate train_data & train_labels for random shuffle\n",
    "    if use_data_augmentation:\n",
    "        train_total_data = expend_training_data(train_data, train_labels)\n",
    "    else:\n",
    "        train_total_data = numpy.concatenate((train_data, train_labels), axis=1)\n",
    "\n",
    "    train_size = train_total_data.shape[0]\n",
    "\n",
    "    return train_total_data, train_size, validation_data, validation_labels, test_data, test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download(filename):\n",
    "    \"\"\"Download the data from Yann's website, unless it's already here.\"\"\"\n",
    "    if not tf.gfile.Exists(DATA_DIRECTORY):\n",
    "        tf.gfile.MakeDirs(DATA_DIRECTORY)\n",
    "    filepath = os.path.join(DATA_DIRECTORY, filename)\n",
    "    if not tf.gfile.Exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "        with tf.gfile.GFile(filepath) as f:\n",
    "            size = f.size()\n",
    "        print('Successfully downloaded', filename, size, 'bytes.')\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture(batch_size, n_dim=2, n_labels=10, x_var=0.5, y_var=0.1, label_indices=None):\n",
    "    if n_dim != 2:\n",
    "        raise Exception(\"n_dim must be 2.\")\n",
    "\n",
    "    def sample(x, y, label, n_labels):\n",
    "        shift = 1.4\n",
    "        r = 2.0 * np.pi / float(n_labels) * float(label)\n",
    "        new_x = x * cos(r) - y * sin(r)\n",
    "        new_y = x * sin(r) + y * cos(r)\n",
    "        new_x += shift * cos(r)\n",
    "        new_y += shift * sin(r)\n",
    "        return np.array([new_x, new_y]).reshape((2,))\n",
    "\n",
    "    x = np.random.normal(0, x_var, (batch_size, (int)(n_dim/2)))\n",
    "    y = np.random.normal(0, y_var, (batch_size, (int)(n_dim/2)))\n",
    "    z = np.empty((batch_size, n_dim), dtype=np.float32)\n",
    "    for batch in range(batch_size):\n",
    "        for zi in range((int)(n_dim/2)):\n",
    "            if label_indices is not None:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], label_indices[batch], n_labels)\n",
    "            else:\n",
    "                z[batch, zi*2:zi*2+2] = sample(x[batch, zi], y[batch, zi], np.random.randint(0, n_labels), n_labels)\n",
    "\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'maybe_download' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0b11c9d7861c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_total_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_MNIST_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6147cfa63f3f>\u001b[0m in \u001b[0;36mprepare_MNIST_data\u001b[0;34m(use_norm_shift, use_norm_scale, use_data_augmentation)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_MNIST_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_norm_shift\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_data_augmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Get the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_data_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_labels_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-labels-idx1-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtest_data_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m't10k-images-idx3-ubyte.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'maybe_download' is not defined"
     ]
    }
   ],
   "source": [
    "train_total_data, train_size, _, _, test_data, test_labels = prepare_MNIST_data()\n",
    "n_samples = train_size  \n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLP_encoder(x, n_hidden, n_output, keep_prob):\n",
    "    with tf.variable_scope(\"MLP_encoder\"):\n",
    "        # initializers\n",
    "        w_init = tf.contrib.layers.xavier_initializer()\n",
    "        b_init = tf.constant_initializer(0.)\n",
    "\n",
    "        # 1st hidden layer\n",
    "        w0 = tf.get_variable('w0', [x.get_shape()[1], n_hidden], initializer=w_init)\n",
    "        b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)\n",
    "        h0 = tf.matmul(x, w0) + b0\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        h0 = tf.nn.dropout(h0, keep_prob)\n",
    "\n",
    "        # 2nd hidden layer\n",
    "        w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)\n",
    "        b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)\n",
    "        h1 = tf.matmul(h0, w1) + b1\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h1 = tf.nn.dropout(h1, keep_prob)\n",
    "\n",
    "        # output layer\n",
    "        wo = tf.get_variable('wo', [h1.get_shape()[1], n_output], initializer=w_init)\n",
    "        bo = tf.get_variable('bo', [n_output], initializer=b_init)\n",
    "        output = tf.matmul(h1, wo) + bo\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP as decoder\n",
    "def MLP_decoder(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "    with tf.variable_scope(\"MLP_decoder\", reuse=reuse):\n",
    "        # initializers\n",
    "        w_init = tf.contrib.layers.xavier_initializer()\n",
    "        b_init = tf.constant_initializer(0.)\n",
    "\n",
    "        # 1st hidden layer\n",
    "        w0 = tf.get_variable('w0', [z.get_shape()[1], n_hidden], initializer=w_init)\n",
    "        b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)\n",
    "        h0 = tf.matmul(z, w0) + b0\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        h0 = tf.nn.dropout(h0, keep_prob)\n",
    "\n",
    "        # 2nd hidden layer\n",
    "        w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)\n",
    "        b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)\n",
    "        h1 = tf.matmul(h0, w1) + b1\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h1 = tf.nn.dropout(h1, keep_prob)\n",
    "\n",
    "        # output layer\n",
    "        wo = tf.get_variable('wo', [h1.get_shape()[1], n_output], initializer=w_init)\n",
    "        bo = tf.get_variable('bo', [n_output], initializer=b_init)\n",
    "        y = tf.sigmoid(tf.matmul(h1, wo) + bo)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(z, n_hidden, n_output, keep_prob, reuse=False):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "        # initializers\n",
    "        w_init = tf.contrib.layers.xavier_initializer()\n",
    "        b_init = tf.constant_initializer(0.)\n",
    "\n",
    "        # 1st hidden layer\n",
    "        w0 = tf.get_variable('w0', [z.get_shape()[1], n_hidden], initializer=w_init)\n",
    "        b0 = tf.get_variable('b0', [n_hidden], initializer=b_init)\n",
    "        h0 = tf.matmul(z, w0) + b0\n",
    "        h0 = tf.nn.relu(h0)\n",
    "        h0 = tf.nn.dropout(h0, keep_prob)\n",
    "\n",
    "        # 2nd hidden layer\n",
    "        w1 = tf.get_variable('w1', [h0.get_shape()[1], n_hidden], initializer=w_init)\n",
    "        b1 = tf.get_variable('b1', [n_hidden], initializer=b_init)\n",
    "        h1 = tf.matmul(h0, w1) + b1\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h1 = tf.nn.dropout(h1, keep_prob)\n",
    "\n",
    "        # output layer\n",
    "        wo = tf.get_variable('wo', [h1.get_shape()[1], n_output], initializer=w_init)\n",
    "        bo = tf.get_variable('bo', [n_output], initializer=b_init)\n",
    "        y = tf.matmul(h1, wo) + bo\n",
    "\n",
    "    return tf.sigmoid(y), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_autoencoder(x_hat, x, x_id, z_sample, z_id, dim_img, dim_z, n_hidden, keep_prob):\n",
    "    # encoding\n",
    "    z = MLP_encoder(x_hat, n_hidden, dim_z, keep_prob)\n",
    "\n",
    "    # decoding\n",
    "    y = MLP_decoder(z, n_hidden, dim_img, keep_prob)\n",
    "\n",
    "    # loss\n",
    "    marginal_likelihood = -tf.reduce_mean(tf.reduce_mean(tf.squared_difference(x,y)))\n",
    "\n",
    "    ## GAN Loss\n",
    "    z_real = tf.concat([z_sample, z_id],1) #z_sample \n",
    "    z_fake = tf.concat([z, x_id],1) #z #t\n",
    "    D_real, D_real_logits = discriminator(z_real, (int)(n_hidden), 1, keep_prob)\n",
    "    D_fake, D_fake_logits = discriminator(z_fake, (int)(n_hidden), 1, keep_prob, reuse=True)\n",
    "\n",
    "    # discriminator loss\n",
    "    D_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real_logits)))\n",
    "    D_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake_logits)))\n",
    "    D_loss = D_loss_real+D_loss_fake\n",
    "\n",
    "    # generator loss\n",
    "    G_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake_logits)))\n",
    "\n",
    "    marginal_likelihood = tf.reduce_mean(marginal_likelihood)\n",
    "    D_loss = tf.reduce_mean(D_loss)\n",
    "    G_loss = tf.reduce_mean(G_loss)\n",
    "\n",
    "    return y, z, -marginal_likelihood, D_loss, G_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: L_tot 1.99 L_likelihood -0.07 d_loss 1.38 g_loss 0.68\n",
      "epoch 1: L_tot 2.00 L_likelihood -0.06 d_loss 1.36 g_loss 0.71\n",
      "epoch 2: L_tot 2.01 L_likelihood -0.06 d_loss 1.39 g_loss 0.68\n",
      "epoch 3: L_tot 2.00 L_likelihood -0.06 d_loss 1.37 g_loss 0.68\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e059fd096c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mmnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-e059fd096c1d>\u001b[0m in \u001b[0;36mmnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m             _, d_loss = sess.run(\n\u001b[1;32m     69\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mtrain_op_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 feed_dict={x_hat: batch_xs_input, x: batch_xs_target, x_id: batch_ids_input, z_sample: samples, z_id: z_id_one_hot_vector,keep_prob: 0.9})\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;31m# generator loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mnist():\n",
    "    x_hat = tf.placeholder(tf.float32, shape=[None, 784], name='input_img')\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784], name='target_img')\n",
    "    x_id = tf.placeholder(tf.float32, shape=[None, 10], name='input_img_label')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    dim_z = 2\n",
    "    dim_img = 784\n",
    "    n_hidden = 400\n",
    "    z_in = tf.placeholder(tf.float32, shape=[None, dim_z], name='latent_variable')\n",
    "    z_id = tf.placeholder(tf.float32, shape=[None, 10], name='prior_sample_label')\n",
    "    z_sample = tf.placeholder(tf.float32, shape=[None, dim_z], name='prior_sample')\n",
    "    y, z, neg_marginal_likelihood, D_loss, G_loss = adversarial_autoencoder(x_hat, x, x_id, z_sample, z_id, dim_img,\n",
    "                                                                                dim_z, n_hidden, keep_prob)\n",
    "    # optimization\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if \"discriminator\" in var.name]\n",
    "    g_vars = [var for var in t_vars if \"MLP_encoder\" in var.name]\n",
    "    ae_vars = [var for var in t_vars if \"MLP_encoder\" or \"MLP_decoder\" in var.name]\n",
    "\n",
    "    learn_rate = 0.001\n",
    "    train_op_ae = tf.train.AdamOptimizer(learn_rate).minimize(neg_marginal_likelihood, var_list=ae_vars)\n",
    "    train_op_d = tf.train.AdamOptimizer(learn_rate/5).minimize(D_loss, var_list=d_vars)\n",
    "    train_op_g = tf.train.AdamOptimizer(learn_rate).minimize(G_loss, var_list=g_vars)\n",
    "    \n",
    "    batch_size = 64\n",
    "    n_epochs = 20\n",
    "    n_samples = 10000 \n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    prior_type = 'normal'\n",
    "    n_labels = 10\n",
    "    dim_z = 2\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer(), feed_dict={keep_prob : 0.9})\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    r_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        # Random shuffling \n",
    "        np.random.shuffle(train_total_data)\n",
    "        train_data_ = train_total_data[:, :-mnist_data.NUM_LABELS]\n",
    "        train_label_ = train_total_data[:, -mnist_data.NUM_LABELS:] # for semi supervised learning\n",
    "\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            # Compute the offset of the current minibatch in the data.\n",
    "            offset = (i * batch_size) % (n_samples)\n",
    "            batch_xs_input = train_data_[offset:(offset + batch_size), :]\n",
    "            batch_ids_input = train_label_[offset:(offset + batch_size), :]\n",
    "            batch_xs_target = batch_xs_input\n",
    "\n",
    "            # draw samples from prior distribution\n",
    "            z_id_ = np.random.randint(0, 10, size=[batch_size])\n",
    "            samples = gaussian_mixture(batch_size, dim_z, label_indices=z_id_)\n",
    "\n",
    "            z_id_one_hot_vector = np.zeros((batch_size, 10))\n",
    "            z_id_one_hot_vector[np.arange(batch_size), z_id_] = 1\n",
    "\n",
    "            # reconstruction loss\n",
    "            _, loss_likelihood = sess.run(\n",
    "                (train_op_ae, neg_marginal_likelihood),\n",
    "                feed_dict={x_hat: batch_xs_input, x: batch_xs_target, x_id: batch_ids_input, z_sample: samples,z_id: z_id_one_hot_vector, keep_prob: 0.9})\n",
    "\n",
    "            # discriminator loss\n",
    "            _, d_loss = sess.run(\n",
    "                (train_op_d, D_loss),\n",
    "                feed_dict={x_hat: batch_xs_input, x: batch_xs_target, x_id: batch_ids_input, z_sample: samples, z_id: z_id_one_hot_vector,keep_prob: 0.9})\n",
    "\n",
    "            # generator loss\n",
    "            for _ in range(2):\n",
    "                _, g_loss = sess.run(\n",
    "                    (train_op_g, G_loss),\n",
    "                    feed_dict={x_hat: batch_xs_input, x: batch_xs_target, x_id: batch_ids_input, z_sample: samples,z_id: z_id_one_hot_vector, keep_prob: 0.9})\n",
    "            g_losses.append(g_loss)\n",
    "            d_losses.append(d_loss)\n",
    "            r_losses.append(-loss_likelihood)\n",
    "        tot_loss = loss_likelihood + d_loss + g_loss\n",
    "\n",
    "        # print cost every epoch\n",
    "        print(\"epoch %d: L_tot %03.2f L_likelihood %03.2f d_loss %03.2f g_loss %03.2f\" % (epoch, tot_loss, loss_likelihood, d_loss, g_loss))\n",
    "    \n",
    "    #now plot the results\n",
    "    plt.figure(1)\n",
    "    plt.plot(g_losses, label = 'generator loss')\n",
    "    plt.figure(2)\n",
    "    plt.plot(d_losses, label = 'discriminator loss')\n",
    "    plt.figure(3)\n",
    "    plt.plot(r_losses, label = 'reconstruction loss')\n",
    "    \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
