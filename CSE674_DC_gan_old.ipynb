{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "import os\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#constants\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('samples'):\n",
    "    os.mkdir('samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, alpha=0.2):\n",
    "    return tf.maximum(alpha*x, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist(limit=None):\n",
    "    df = pd.read_csv('train.csv')\n",
    "    data = df.as_matrix()\n",
    "    np.random.shuffle(data)\n",
    "    X = data[:, 1:] / 255.0 # data is from 0..255\n",
    "    Y = data[:, 0]\n",
    "    if limit is not None:\n",
    "        X, Y = X[:limit], Y[:limit]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, name, mi, mo, apply_batch_norm, filtersz=5, stride=2, f=tf.nn.relu):\n",
    "        # mi = input image size\n",
    "        # mo = output image size\n",
    "        self.W = tf.get_variable(\n",
    "          \"W_%s\" % name,\n",
    "          shape=(filtersz, filtersz, mi, mo),\n",
    "          initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
    "        )\n",
    "        self.b = tf.get_variable(\n",
    "          \"b_%s\" % name,\n",
    "          shape=(mo,),\n",
    "          initializer=tf.zeros_initializer(),\n",
    "        )\n",
    "        self.name = name\n",
    "        self.f = f\n",
    "        self.stride = stride\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X, reuse, is_training):\n",
    "        conv_out = tf.nn.conv2d(\n",
    "          X,\n",
    "          self.W,\n",
    "          strides=[1, self.stride, self.stride, 1],\n",
    "          padding='SAME'\n",
    "        )\n",
    "        conv_out = tf.nn.bias_add(conv_out, self.b)\n",
    "\n",
    "        # apply batch normalization\n",
    "        if self.apply_batch_norm:\n",
    "            conv_out = tf.contrib.layers.batch_norm(\n",
    "                conv_out,\n",
    "                decay=0.9, \n",
    "                updates_collections=None,\n",
    "                epsilon=1e-5,\n",
    "                scale=True,\n",
    "                is_training=is_training,\n",
    "                reuse=reuse,\n",
    "                scope=self.name,\n",
    "          )\n",
    "        return self.f(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeConvLayer:\n",
    "    def __init__(self, name, mi, mo, output_shape, apply_batch_norm, filtersz=5, stride=2, f=tf.nn.relu):\n",
    "        # mi = input input size\n",
    "        # mo = output image size\n",
    "        self.W = tf.get_variable(\n",
    "          \"W_%s\" % name,\n",
    "          shape=(filtersz, filtersz, mo, mi),\n",
    "          initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        )\n",
    "        self.b = tf.get_variable(\n",
    "          \"b_%s\" % name,\n",
    "          shape=(mo,),\n",
    "          initializer=tf.zeros_initializer(),\n",
    "        )\n",
    "        self.f = f\n",
    "        self.stride = stride\n",
    "        self.name = name\n",
    "        self.output_shape = output_shape\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X, reuse, is_training):\n",
    "        conv_out = tf.nn.conv2d_transpose(\n",
    "          value=X,\n",
    "          filter=self.W,\n",
    "          output_shape=self.output_shape,\n",
    "          strides=[1, self.stride, self.stride, 1],\n",
    "        )\n",
    "        conv_out = tf.nn.bias_add(conv_out, self.b)\n",
    "\n",
    "        # apply batch normalization\n",
    "        if self.apply_batch_norm:\n",
    "            conv_out = tf.contrib.layers.batch_norm(\n",
    "                conv_out,\n",
    "                decay=0.9, \n",
    "                updates_collections=None,\n",
    "                epsilon=1e-5,\n",
    "                scale=True,\n",
    "                is_training=is_training,\n",
    "                reuse=reuse,\n",
    "                scope=self.name,\n",
    "          )\n",
    "\n",
    "        return self.f(conv_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(object):\n",
    "    def __init__(self, name, M1, M2, apply_batch_norm, f=tf.nn.relu):\n",
    "        self.W = tf.get_variable(\n",
    "          \"W_%s\" % name,\n",
    "          shape=(M1, M2),\n",
    "          initializer=tf.random_normal_initializer(stddev=0.02),\n",
    "        )\n",
    "        self.b = tf.get_variable(\n",
    "          \"b_%s\" % name,\n",
    "          shape=(M2,),\n",
    "          initializer=tf.zeros_initializer(),\n",
    "        )\n",
    "        self.f = f\n",
    "        self.name = name\n",
    "        self.apply_batch_norm = apply_batch_norm\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X, reuse, is_training):\n",
    "        a = tf.matmul(X, self.W) + self.b\n",
    "\n",
    "        # apply batch normalization\n",
    "        if self.apply_batch_norm:\n",
    "            a = tf.contrib.layers.batch_norm(\n",
    "                a,\n",
    "                decay=0.9, \n",
    "                updates_collections=None,\n",
    "                epsilon=1e-5,\n",
    "                scale=True,\n",
    "                is_training=is_training,\n",
    "                reuse=reuse,\n",
    "                scope=self.name,\n",
    "          )\n",
    "        return self.f(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "    def __init__(self, img_length, num_colors, d_sizes, g_sizes):\n",
    "\n",
    "        # save for later\n",
    "        self.img_length = img_length\n",
    "        self.num_colors = num_colors\n",
    "        self.latent_dims = g_sizes['z']\n",
    "\n",
    "        # define the input data\n",
    "        self.X = tf.placeholder(\n",
    "          tf.float32,\n",
    "          shape=(None, img_length, img_length, num_colors),\n",
    "          name='X'\n",
    "        )\n",
    "        self.Z = tf.placeholder(\n",
    "          tf.float32,\n",
    "          shape=(None, self.latent_dims),\n",
    "          name='Z'\n",
    "        )\n",
    "\n",
    "        self.batch_sz = tf.placeholder(tf.int32, shape=(), name='batch_sz')\n",
    "\n",
    "        # build the discriminator\n",
    "        logits = self.build_discriminator(self.X, d_sizes)\n",
    "\n",
    "        # build generator\n",
    "        self.sample_images = self.build_generator(self.Z, g_sizes)\n",
    "\n",
    "        # get sample logits\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            scope.reuse_variables()\n",
    "            sample_logits = self.d_forward(self.sample_images, True)\n",
    "\n",
    "        # get sample images for test time (batch norm is different)\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            scope.reuse_variables()\n",
    "            self.sample_images_test = self.g_forward(\n",
    "            self.Z, reuse=True, is_training=False\n",
    "            )\n",
    "\n",
    "        # build costs\n",
    "        self.d_cost_real = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=logits,\n",
    "          labels=tf.ones_like(logits)\n",
    "        )\n",
    "        self.d_cost_fake = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "          logits=sample_logits,\n",
    "          labels=tf.zeros_like(sample_logits)\n",
    "        )\n",
    "        self.d_cost = tf.reduce_mean(self.d_cost_real) + tf.reduce_mean(self.d_cost_fake)\n",
    "        self.g_cost = tf.reduce_mean(\n",
    "          tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=sample_logits,\n",
    "            labels=tf.ones_like(sample_logits)\n",
    "          )\n",
    "        )\n",
    "        real_predictions = tf.cast(logits > 0, tf.float32)\n",
    "        fake_predictions = tf.cast(sample_logits < 0, tf.float32)\n",
    "        num_predictions = 2.0*BATCH_SIZE\n",
    "        num_correct = tf.reduce_sum(real_predictions) + tf.reduce_sum(fake_predictions)\n",
    "        self.d_accuracy = num_correct / num_predictions\n",
    "\n",
    "\n",
    "        # optimizers\n",
    "        self.d_params = [t for t in tf.trainable_variables() if t.name.startswith('d')]\n",
    "        self.g_params = [t for t in tf.trainable_variables() if t.name.startswith('g')]\n",
    "\n",
    "        self.d_train_op = tf.train.AdamOptimizer(\n",
    "         0.001\n",
    "        ).minimize(\n",
    "          self.d_cost, var_list=self.d_params\n",
    "        )\n",
    "        self.g_train_op = tf.train.AdamOptimizer(\n",
    "          0.001\n",
    "        ).minimize(\n",
    "          self.g_cost, var_list=self.g_params\n",
    "        )\n",
    "\n",
    "        # show_all_variables()\n",
    "        # exit()\n",
    "\n",
    "        # set up session and variables for later\n",
    "        self.init_op = tf.global_variables_initializer()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(self.init_op)\n",
    "\n",
    "\n",
    "    def build_discriminator(self, X, d_sizes):\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "\n",
    "            # build conv layers\n",
    "            self.d_convlayers = []\n",
    "            mi = self.num_colors\n",
    "            dim = self.img_length\n",
    "            count = 0\n",
    "            for mo, filtersz, stride, apply_batch_norm in d_sizes['conv_layers']:\n",
    "                # make up a name - used for get_variable\n",
    "                name = \"convlayer_%s\" % count\n",
    "                count += 1\n",
    "\n",
    "                layer = ConvLayer(name, mi, mo, apply_batch_norm, filtersz, stride, lrelu)\n",
    "                self.d_convlayers.append(layer)\n",
    "                mi = mo\n",
    "                print(\"dim:\", dim)\n",
    "                dim = int(np.ceil(float(dim) / stride))\n",
    "\n",
    "\n",
    "            mi = mi * dim * dim\n",
    "            # build dense layers\n",
    "            self.d_denselayers = []\n",
    "            for mo, apply_batch_norm in d_sizes['dense_layers']:\n",
    "                name = \"denselayer_%s\" % count\n",
    "                count += 1\n",
    "\n",
    "                layer = DenseLayer(name, mi, mo, apply_batch_norm, lrelu)\n",
    "                mi = mo\n",
    "                self.d_denselayers.append(layer)\n",
    "\n",
    "\n",
    "            # final logistic layer\n",
    "            name = \"denselayer_%s\" % count\n",
    "            self.d_finallayer = DenseLayer(name, mi, 1, False, lambda x: x)\n",
    "\n",
    "            # get the logits\n",
    "            logits = self.d_forward(X)\n",
    "\n",
    "            # build the cost later\n",
    "            return logits\n",
    "\n",
    "\n",
    "    def d_forward(self, X, reuse=None, is_training=True):\n",
    "        output = X\n",
    "        for layer in self.d_convlayers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "        output = tf.contrib.layers.flatten(output)\n",
    "        for layer in self.d_denselayers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "        logits = self.d_finallayer.forward(output, reuse, is_training)\n",
    "        return logits\n",
    "\n",
    "\n",
    "    def build_generator(self, Z, g_sizes):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            dims = [self.img_length]\n",
    "            dim = self.img_length\n",
    "            for _, _, stride, _ in reversed(g_sizes['conv_layers']):\n",
    "                dim = int(np.ceil(float(dim) / stride))\n",
    "                dims.append(dim)\n",
    "\n",
    "            #for generator dimension is reverse\n",
    "            dims = list(reversed(dims))\n",
    "            print(\"dims:\", dims)\n",
    "            self.g_dims = dims\n",
    "\n",
    "\n",
    "            # dense layers\n",
    "            mi = self.latent_dims\n",
    "            self.g_denselayers = []\n",
    "            count = 0\n",
    "            for mo, apply_batch_norm in g_sizes['dense_layers']:\n",
    "                name = \"g_denselayer_%s\" % count\n",
    "                count += 1\n",
    "\n",
    "                layer = DenseLayer(name, mi, mo, apply_batch_norm)\n",
    "                self.g_denselayers.append(layer)\n",
    "                mi = mo\n",
    "\n",
    "            # final dense layer\n",
    "            mo = g_sizes['projection'] * dims[0] * dims[0]\n",
    "            name = \"g_denselayer_%s\" % count\n",
    "            layer = DenseLayer(name, mi, mo, not g_sizes['bn_after_project'])\n",
    "            self.g_denselayers.append(layer)\n",
    "\n",
    "\n",
    "            # de-conv layers\n",
    "            mi = g_sizes['projection']\n",
    "            self.g_convlayers = []\n",
    "\n",
    "            num_relus = len(g_sizes['conv_layers']) - 1\n",
    "            activation_functions = [tf.nn.relu]*num_relus + [g_sizes['output_activation']]\n",
    "\n",
    "            for i in range(len(g_sizes['conv_layers'])):\n",
    "                name = \"de_convlayer_%s\" % i\n",
    "                mo, filtersz, stride, apply_batch_norm = g_sizes['conv_layers'][i]\n",
    "                f = activation_functions[i]\n",
    "                output_shape = [self.batch_sz, dims[i+1], dims[i+1], mo]\n",
    "                print(\"mi:\", mi, \"mo:\", mo, \"outp shape:\", output_shape)\n",
    "                layer = DeConvLayer(\n",
    "                  name, mi, mo, output_shape, apply_batch_norm, filtersz, stride, f\n",
    "                )\n",
    "                self.g_convlayers.append(layer)\n",
    "                mi = mo\n",
    "\n",
    "            # get the output\n",
    "            self.g_sizes = g_sizes\n",
    "            return self.g_forward(Z)\n",
    "\n",
    "\n",
    "    def g_forward(self, Z, reuse=None, is_training=True):\n",
    "        # dense layers\n",
    "        output = Z\n",
    "        for layer in self.g_denselayers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "\n",
    "        # project and reshape\n",
    "        output = tf.reshape(\n",
    "          output,\n",
    "          [-1, self.g_dims[0], self.g_dims[0], self.g_sizes['projection']],\n",
    "        )\n",
    "\n",
    "        # apply batch norm\n",
    "        if self.g_sizes['bn_after_project']:\n",
    "            output = tf.contrib.layers.batch_norm(\n",
    "            output,\n",
    "            decay=0.9, \n",
    "            updates_collections=None,\n",
    "            epsilon=1e-5,\n",
    "            scale=True,\n",
    "            is_training=is_training,\n",
    "            reuse=reuse,\n",
    "            scope='bn_after_project'\n",
    "          )\n",
    "\n",
    "        # forward the de-conv layers\n",
    "        for layer in self.g_convlayers:\n",
    "            output = layer.forward(output, reuse, is_training)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        d_costs = []\n",
    "        g_costs = []\n",
    "\n",
    "        N = len(X)\n",
    "        n_batches = N // BATCH_SIZE\n",
    "        total_iters = 0\n",
    "        for i in range(EPOCHS):\n",
    "            print(\"epoch:\", i)\n",
    "            np.random.shuffle(X)\n",
    "            for j in range(n_batches):\n",
    "                t0 = datetime.now()\n",
    "                \n",
    "                # get the  mnist dataset\n",
    "                batch = X[j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n",
    "\n",
    "                Z = np.random.uniform(-1, 1, size=(BATCH_SIZE, self.latent_dims))\n",
    "\n",
    "                # train the discriminator\n",
    "                _, d_cost, d_acc = self.sess.run(\n",
    "                  (self.d_train_op, self.d_cost, self.d_accuracy),\n",
    "                  feed_dict={self.X: batch, self.Z: Z, self.batch_sz: BATCH_SIZE},\n",
    "                )\n",
    "                d_costs.append(d_cost)\n",
    "\n",
    "                # train the generator\n",
    "                _, g_cost1 = self.sess.run(\n",
    "                  (self.g_train_op, self.g_cost),\n",
    "                  feed_dict={self.Z: Z, self.batch_sz: BATCH_SIZE},\n",
    "                )\n",
    "                # g_costs.append(g_cost1)\n",
    "                _, g_cost2 = self.sess.run(\n",
    "                  (self.g_train_op, self.g_cost),\n",
    "                  feed_dict={self.Z: Z, self.batch_sz: BATCH_SIZE},\n",
    "                )\n",
    "                g_costs.append((g_cost1 + g_cost2)/2) # \n",
    "                \n",
    "        # save a plot of the costs\n",
    "        plt.clf()\n",
    "        plt.plot(d_costs, label='discriminator cost')\n",
    "        plt.plot(g_costs, label='generator cost')\n",
    "        plt.legend()\n",
    "        plt.savefig('cost_vs_iteration.png')\n",
    "\n",
    "    def sample(self, n):\n",
    "        Z = np.random.uniform(-1, 1, size=(n, self.latent_dims))\n",
    "        samples = self.sess.run(self.sample_images_test, feed_dict={self.Z: Z, self.batch_sz: n})\n",
    "        return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim: 28\n",
      "dim: 14\n",
      "dims: [7, 14, 28]\n",
      "mi: 128 mo: 128 outp shape: [<tf.Tensor 'batch_sz:0' shape=() dtype=int32>, 14, 14, 128]\n",
      "mi: 128 mo: 1 outp shape: [<tf.Tensor 'batch_sz:0' shape=() dtype=int32>, 28, 28, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-3e9bfb020589>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmnist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-3e9bfb020589>\u001b[0m in \u001b[0;36mmnist\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#initialize gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msample_image\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-b57c0e56d410>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 _, g_cost2 = self.sess.run(\n\u001b[1;32m    263\u001b[0m                   \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_train_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m                 )\n\u001b[1;32m    266\u001b[0m                 \u001b[0mg_costs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_cost1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mg_cost2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def mnist():    \n",
    "    X, Y = get_mnist()\n",
    "    X = X.reshape(len(X), 28, 28, 1)\n",
    "    dim = X.shape[1]\n",
    "    colors = X.shape[-1]\n",
    "\n",
    "    # for mnist\n",
    "    d_sizes = {\n",
    "    'conv_layers': [(64, 5, 2, False), (128, 5, 2, True), (256, 5, 2, True), (512, 5, 2, True)],\n",
    "    'dense_layers': [(1024, True)],\n",
    "    }\n",
    "    g_sizes = {\n",
    "    'z': 100,\n",
    "    'projection': 128,\n",
    "    'bn_after_project': False,\n",
    "    'conv_layers': [(128, 5, 2, True), (colors, 5, 2, False)],\n",
    "    'dense_layers': [(1024, True)],\n",
    "    'output_activation': tf.sigmoid,\n",
    "    }\n",
    "\n",
    "\n",
    "    #initialize gan\n",
    "    gan = DCGAN(dim, colors, d_sizes, g_sizes)\n",
    "    gan.fit(X)\n",
    "    sample_image  = gan.sample(1)\n",
    "\n",
    "    #now show the image\n",
    "    sample_image = np.squeeze(sample_image)\n",
    "    plt.imshow(sample_image)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
